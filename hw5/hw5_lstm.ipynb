{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy.random\n",
    "from torch.utils.data import dataloader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training_firsts (and seconds) will be a lists of length 6036 each. Each element of these lists is a sentence vector, where each element corresponds to the words in the sentence. both `<s>` and `</s>` are included in each sentence vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "training_size = 6036\n",
    "training_firsts = []\n",
    "training_secs = []\n",
    "with open(\"bobsue-data/bobsue.seq2seq.train.tsv\") as trainfile:\n",
    "    for line in trainfile:\n",
    "        separated = line.split()\n",
    "        for i in range(len(separated)):\n",
    "            if separated[i] == \"</s>\":\n",
    "                sent1 = separated[:i + 1]\n",
    "                sent2 = separated[i + 1:]\n",
    "                break\n",
    "        training_firsts.append(sent1)\n",
    "        training_secs.append(sent2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we repeat for validation and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATION\n",
    "validation_size = 750\n",
    "valid_firsts = []\n",
    "valid_secs = []\n",
    "with open(\"bobsue-data/bobsue.seq2seq.dev.tsv\") as valfile:\n",
    "    for line in valfile:\n",
    "        separated = line.split()\n",
    "        for i in range(len(separated)):\n",
    "            if separated[i] == \"</s>\":\n",
    "                sent1 = separated[:i + 1]\n",
    "                sent2 = separated[i + 1:]\n",
    "                break\n",
    "        valid_firsts.append(sent1)\n",
    "        valid_secs.append(sent2)\n",
    "\n",
    "# TESTING\n",
    "testing_size = 750\n",
    "test_firsts = []\n",
    "test_secs = []\n",
    "with open(\"bobsue-data/bobsue.seq2seq.test.tsv\") as testfile:\n",
    "    for line in testfile:\n",
    "        separated = line.split()\n",
    "        for i in range(len(separated)):\n",
    "            if separated[i] == \"</s>\":\n",
    "                sent1 = separated[:i + 1]\n",
    "                sent2 = separated[i + 1:]\n",
    "                break\n",
    "        test_firsts.append(sent1)\n",
    "        test_secs.append(sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to give each word in our target vocabulary an embedding. To do this, we'll use GloVe. I'll be using snippits from [Chris Olah's Blog](https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76) in my implementation. `word_2_index` maps each word in the GloVe file to an index (i.e. 0, 1, ...). Associated with each index is the corresponding entry in `embeddings_vector`. `embeddings_vector` has shape `(size of glove vocabulary, EMBED_SIZE)`. `EMBED_SIZE` will eventually be 200, but for now it is 50 to speed up training. `glove_voc` stores each word in the GloVe vocabulary. Lastly, we'll create a dictionary `glove` that has keys that are vocabulary words, and the values are the corresponding embedding vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GloVe\n",
    "#TODO: Change embedding size from 50 to 200\n",
    "EMBED_SIZE = 50\n",
    "#See descriptions above for the below three variables\n",
    "word_to_index = {}\n",
    "embeddings_vector = []\n",
    "glove_voc = []\n",
    "\n",
    "index = 0\n",
    "with open(f'embedding-data/glove.6B.{EMBED_SIZE}d.txt') as embedfile:\n",
    "    for line in embedfile:\n",
    "        #Store the first element of the line as the word, store the remaining elements as the embedding vector of size EMBED_SIZE\n",
    "        split_vector = line.split()\n",
    "        cur_embedded_word = split_vector[0]\n",
    "        \n",
    "        cur_embedded_vector = np.array(split_vector[1:]).astype(np.double)\n",
    "        word_to_index[cur_embedded_word] = index\n",
    "\n",
    "        glove_voc.append(cur_embedded_word)\n",
    "        embeddings_vector.append(cur_embedded_vector)\n",
    "        index += 1\n",
    "\n",
    "glove = {w: embeddings_vector[word_to_index[w]] for w in glove_voc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll generate our target vocabulary from our `bobsue` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-process target vocabulary\n",
    "target_voc = []\n",
    "with open('bobsue-data/bobsue.voc.txt') as vocfile:\n",
    "    for line in vocfile:\n",
    "        line = line.split()\n",
    "        target_voc.append(line[0])\n",
    "target_voc = np.array(target_voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to ensure that we have embeddings for all words in our target vocabulary. To do this, we'll loop through each word in `target_voc`, and if the word appears in our glove dictionary with an embedding, we link that same embedding in the new `target_embeddings` dictionary. If not, we assign a normally distributed random vector of dimension EMBED_SIZE. `target_voc` is a `numpy` array where each element is a word that appears in our training/testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Remove me\n",
    "np.random.seed(0)\n",
    "tgt_vocab_len = len(target_voc)\n",
    "target_embeddings = {}\n",
    "for i in range(tgt_vocab_len):\n",
    "    cur_word = target_voc[i]\n",
    "    if cur_word in glove:\n",
    "        target_embeddings[cur_word] = glove[cur_word]\n",
    "    else:\n",
    "        target_embeddings[cur_word] = np.random.normal(scale=0.6, size=(EMBED_SIZE, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary `target_to_index` from a target word to its corresponding index (does the inverse of calling `target_voc[i]`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_to_index = {}\n",
    "for i, word in enumerate(target_voc):\n",
    "    target_to_index[word] = i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
