{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence-to-Sequence LSTM: Predicting the Second Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM model implemented in this repo draws from the [Sequence to Sequence Learning with Neural Networks (2014)](https://arxiv.org/pdf/1409.3215) paper from 2014 written by Google researchers Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. The original paper uses an encoder/decoder LSTM model to convert English to French. Ben Trevett, an NLP engineer, implemented a similar but simplified seq-2-seq model based on the Google paper. There are many code segments that I'm using from Ben's implementation that can be found in his GitHub repo [here](https://github.com/bentrevett/pytorch-seq2seq).\n",
    "\n",
    "The main difference between my code and Ben's code is that I'll be implementing a version of `nn.LSTM` myself, rather than using pytorch's pre-built implementation. Some additional differences in my implementation:\n",
    "\n",
    "- Source vocabulary and target vocabulary are identical (as nothing is being translated)\n",
    "- Doing all dataset creation/preprocessing myself\n",
    "- Running everything on CPU vs. GPU\n",
    "\n",
    "I also found a handful of other articles/videos really useful:\n",
    "\n",
    "- [PyTorch LSTM documentation](\"https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\"): This was useful in understanding what my LSTMCell class needed to have in order to function with my Encoder, Decoder, and S2S classes as well as working with PyTorch's autodifferentiation, SGD, and cross entropy loss functions.\n",
    "\n",
    "- [StatQuest YouTube Channel](\"https://www.youtube.com/watch?v=L8HKweZIOmg&t=24s&ab_channel=StatQuestwithJoshStarmer\"): StatQuest YouTube channel has a bunch of videos on sequence to sequence problems, which helped with the intuitive understanding. He also has videos that implement seq-2-seq using PyTorch Lightning. He follow's Ben's repository as well.\n",
    "\n",
    "- [Mustafa Murat ARAT Blog](\"https://mmuratarat.github.io/2019-01-19/dimensions-of-lstm\"): This article was helpful in understanding the dimensionality of weights/biases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/will/miniconda3/envs/pycourse/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/will/miniconda3/envs/pycourse/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/will/miniconda3/envs/pycourse/lib/python3.8/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import evaluate\n",
    "\n",
    "#TODO: Change embedding size from 50 to 200\n",
    "EMBED_SIZE = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set all random seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe055092f50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-process the Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training_firsts (and seconds) will be a lists of length 6036 each. Each element of these lists is a sentence vector, where each element corresponds to the words in the sentence. both `<s>` and `</s>` are included in each sentence vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "training_size = 6036\n",
    "training_firsts = []\n",
    "training_secs = []\n",
    "with open(\"bobsue-data/bobsue.seq2seq.train.tsv\") as trainfile:\n",
    "    for line in trainfile:\n",
    "        separated = line.split()\n",
    "        for i in range(len(separated)):\n",
    "            if separated[i] == \"</s>\":\n",
    "                sent1 = separated[:i + 1]\n",
    "                sent2 = separated[i + 1:]\n",
    "                break\n",
    "        training_firsts.append(sent1)\n",
    "        training_secs.append(sent2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we repeat for validation and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATION\n",
    "validation_size = 750\n",
    "valid_firsts = []\n",
    "valid_secs = []\n",
    "with open(\"bobsue-data/bobsue.seq2seq.dev.tsv\") as valfile:\n",
    "    for line in valfile:\n",
    "        separated = line.split()\n",
    "        for i in range(len(separated)):\n",
    "            if separated[i] == \"</s>\":\n",
    "                sent1 = separated[:i + 1]\n",
    "                sent2 = separated[i + 1:]\n",
    "                break\n",
    "        valid_firsts.append(sent1)\n",
    "        valid_secs.append(sent2)\n",
    "\n",
    "# TESTING\n",
    "testing_size = 750\n",
    "test_firsts = []\n",
    "test_secs = []\n",
    "with open(\"bobsue-data/bobsue.seq2seq.test.tsv\") as testfile:\n",
    "    for line in testfile:\n",
    "        separated = line.split()\n",
    "        for i in range(len(separated)):\n",
    "            if separated[i] == \"</s>\":\n",
    "                sent1 = separated[:i + 1]\n",
    "                sent2 = separated[i + 1:]\n",
    "                break\n",
    "        test_firsts.append(sent1)\n",
    "        test_secs.append(sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Generating Embeddings: GloVe\n",
    "\n",
    "We now have to give each word in our target vocabulary an embedding. To do this, we'll use GloVe. I'll be using snippits from [Martín Pellarolo’s Blog](https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76) in my implementation. `word_2_index` maps each word in the GloVe file to an index (i.e. 0, 1, ...). Associated with each index is the corresponding entry in `embeddings_vector`. `embeddings_vector` has shape `(size of glove vocabulary, EMBED_SIZE)`. `EMBED_SIZE` will eventually be 200, but for now it is 50 to speed up training. `glove_voc` stores each word in the GloVe vocabulary. Lastly, we'll create a dictionary `glove` that has keys that are vocabulary words, and the values are the corresponding embedding vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GloVe\n",
    "#See descriptions above for the below three variables\n",
    "word_to_index = {}\n",
    "embeddings_vector = []\n",
    "glove_voc = []\n",
    "\n",
    "index = 0\n",
    "with open(f'embedding-data/glove.6B.{EMBED_SIZE}d.txt') as embedfile:\n",
    "    for line in embedfile:\n",
    "        #Store the first element of the line as the word, store the remaining elements as the embedding vector of size EMBED_SIZE\n",
    "        split_vector = line.split()\n",
    "        cur_embedded_word = split_vector[0]\n",
    "        \n",
    "        cur_embedded_vector = np.array(split_vector[1:]).astype(np.float32)\n",
    "        word_to_index[cur_embedded_word] = index\n",
    "\n",
    "        glove_voc.append(cur_embedded_word)\n",
    "        embeddings_vector.append(cur_embedded_vector)\n",
    "        index += 1\n",
    "\n",
    "glove = {w: embeddings_vector[word_to_index[w]] for w in glove_voc}\n",
    "index_to_add = index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll generate our target vocabulary from our `bobsue.voc.txt` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-process target vocabulary\n",
    "target_voc = []\n",
    "with open('bobsue-data/bobsue.voc.txt') as vocfile:\n",
    "    for line in vocfile:\n",
    "        line = line.split()\n",
    "        target_voc.append(line[0])\n",
    "target_voc = np.array(target_voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to ensure that we have embeddings for all words in our target vocabulary. To do this, we'll loop through each word in `target_voc`, and if the word appears in our glove dictionary with an embedding, we link that same embedding in the new `target_embeddings` dictionary. If not, we assign a normally distributed random vector of dimension EMBED_SIZE. `target_voc` is a `numpy` array where each element is a word that appears in our training/testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_vocab_len = len(target_voc)\n",
    "target_embeddings = {}\n",
    "for i in range(tgt_vocab_len):\n",
    "    cur_word = target_voc[i]\n",
    "    if cur_word in glove:\n",
    "        target_embeddings[cur_word] = glove[cur_word]\n",
    "    else:\n",
    "        target_embeddings[cur_word] = np.random.normal(scale=0.6, size=(EMBED_SIZE, ))\n",
    "        word_to_index[cur_word] = index_to_add\n",
    "        index_to_add += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary `target_to_index` from a target word to its corresponding index (does the inverse of calling `target_voc[i]`). Then, we'll define `tokenized_to_indices` to convert a tokenized sentence into a list of integers corresponding to the index in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_to_index = {}\n",
    "index_to_target = {}\n",
    "index_to_embedding = []\n",
    "for i, word in enumerate(target_voc):\n",
    "    target_to_index[word] = i\n",
    "    index_to_target[i] = word\n",
    "\n",
    "for i in range(len(target_voc)):\n",
    "    target = index_to_target[i]\n",
    "    embedding = target_embeddings[target]\n",
    "    index_to_embedding.append(embedding)\n",
    "\n",
    "index_to_embedding = np.array(index_to_embedding)\n",
    "index_to_embedding = torch.from_numpy(index_to_embedding).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_to_indices(tokens):\n",
    "    indices = []\n",
    "    for i in range(len(tokens)):\n",
    "        indices.append(target_to_index[tokens[i]])\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert all of our training sentences into tensors of numerical embeddings. First we'll write a function that takes a tokenized sentence and outputs the corresponding tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_embed(tokenized_seq):\n",
    "    \"\"\"\n",
    "    Given a tokenized sentence (where tokenized seq is a vector of tokens for a given sentence),\n",
    "    returns the a vector where each element corresponds to the embedding of that word\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for token in tokenized_seq:\n",
    "        cur_embedding = target_embeddings[token]\n",
    "        embeddings.append(cur_embedding)\n",
    "    embeddings = np.array(embeddings)\n",
    "    embeddings = torch.from_numpy(embeddings).float()\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating our data:\n",
    "Now, we'll create full datasets for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Data\n",
    "training_data = []\n",
    "for i in range(len(training_firsts)):\n",
    "    cur_pair_data = {\n",
    "        'src_tokens' : training_firsts[i],\n",
    "        'tgt_tokens' : training_secs[i],\n",
    "\n",
    "        'src_len' : len(training_firsts[i]),\n",
    "        'tgt_len' : len(training_secs[i]),\n",
    "\n",
    "        'src_indices' : torch.tensor(tokenized_to_indices(training_firsts[i]), dtype=torch.int16),\n",
    "        'tgt_indices' : torch.tensor(tokenized_to_indices(training_secs[i]), dtype=torch.int16),\n",
    "\n",
    "        'src_embeddings' : sent_to_embed(training_firsts[i]),\n",
    "        'tgt_embeddings' : sent_to_embed(training_secs[i])\n",
    "        }\n",
    "    training_data.append(cur_pair_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll repeat the process for validation and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VALIDATION DATA\n",
    "validation_data = []\n",
    "for i in range(len(valid_firsts)):\n",
    "    cur_pair_data = {\n",
    "        'src_tokens' : valid_firsts[i],\n",
    "        'tgt_tokens' : valid_secs[i],\n",
    "\n",
    "        'src_len' : len(valid_firsts[i]),\n",
    "        'tgt_len' : len(valid_secs[i]),\n",
    "\n",
    "        'src_indices' : torch.tensor(tokenized_to_indices(valid_firsts[i]), dtype=torch.int16),\n",
    "        'tgt_indices' : torch.tensor(tokenized_to_indices(valid_secs[i]), dtype=torch.int16),\n",
    "\n",
    "        'src_embeddings' : sent_to_embed(valid_firsts[i]),\n",
    "        'tgt_embeddings' : sent_to_embed(valid_secs[i])\n",
    "        }\n",
    "    validation_data.append(cur_pair_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING DATA\n",
    "testing_data = []\n",
    "for i in range(len(test_firsts)):\n",
    "    cur_pair_data = {\n",
    "        'src_tokens' : test_firsts[i],\n",
    "        'tgt_tokens' : test_secs[i],\n",
    "\n",
    "        'src_len' : len(test_firsts[i]),\n",
    "        'tgt_len' : len(test_secs[i]),\n",
    "\n",
    "        'src_indices' : torch.tensor(tokenized_to_indices(test_firsts[i]), dtype=torch.int16),\n",
    "        'tgt_indices' : torch.tensor(tokenized_to_indices(test_secs[i]), dtype=torch.int16),\n",
    "\n",
    "        'src_embeddings' : sent_to_embed(test_firsts[i]),\n",
    "        'tgt_embeddings' : sent_to_embed(test_secs[i])\n",
    "        }\n",
    "    testing_data.append(cur_pair_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model\n",
    "We've now prepared our training, validation, and testing data. We now need to build the LSTM model, Encoder, and Decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Encoder, Decoder, S2S\n",
    "\n",
    "#See model.py for encoder, decoder, and seq-2-seq classes\n",
    "\n",
    "encoder = Encoder(EMBED_SIZE, EMBED_SIZE)\n",
    "decoder = Decoder(EMBED_SIZE, EMBED_SIZE, index_to_embedding)\n",
    "s2s = S2S(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initalize weights: the paper uses uniform between -0.08 and 0.08. Set up optimizer and loss functions: I'm using SGD and cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(model):\n",
    "    for name, parameter in model.named_parameters():\n",
    "        nn.init.uniform_(parameter.data, -0.08, 0.08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, criterion, training_data):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i in range(len(training_data)):\n",
    "        src = training_data[i]['src_embeddings']\n",
    "        tgt = training_data[i]['tgt_embeddings']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt, tf=True)\n",
    "        \n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss/len(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Loop: (no updating parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(model, criterion, val_data):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(val_data)):\n",
    "            src = val_data[i]['src_embeddings']\n",
    "            tgt = val_data[i]['tgt_embeddings']\n",
    "\n",
    "            output = model(src, tgt, tf=False)\n",
    "\n",
    "            loss = criterion(output, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss/len(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:44<14:00, 44.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.1745582824749434\n",
      "\tValid Loss: 0.1540367602010568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [01:27<13:10, 43.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.1517414031408337\n",
      "\tValid Loss: 0.15026627372701962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [02:11<12:27, 43.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.1499729062757048\n",
      "\tValid Loss: 0.14953332488735516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [02:55<11:43, 43.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.14948629937335267\n",
      "\tValid Loss: 0.14921926350394885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [03:40<11:01, 44.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.14924976373593724\n",
      "\tValid Loss: 0.14903842722376187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [04:24<10:16, 44.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.14907877605761458\n",
      "\tValid Loss: 0.14891382584969204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [05:08<09:31, 43.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.1489695454839644\n",
      "\tValid Loss: 0.14883095063765844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [05:51<08:46, 43.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.1488892997326817\n",
      "\tValid Loss: 0.14878390829761823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [06:34<07:58, 43.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.14882898636809083\n",
      "\tValid Loss: 0.1487340432802836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [07:18<07:17, 43.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.14877066427485053\n",
      "\tValid Loss: 0.14869056687752405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [08:02<06:33, 43.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.14872337398943206\n",
      "\tValid Loss: 0.14864489895105362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [08:45<05:47, 43.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.1486859311657662\n",
      "\tValid Loss: 0.14862359127402305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [09:28<05:03, 43.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.14866132123150164\n",
      "\tValid Loss: 0.1486117363870144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [10:11<04:20, 43.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.14863962130442093\n",
      "\tValid Loss: 0.14858598934610684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [10:56<03:38, 43.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.14862246743623708\n",
      "\tValid Loss: 0.1485744778017203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [11:39<02:54, 43.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.1486056306188947\n",
      "\tValid Loss: 0.1485588468015194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [12:22<02:10, 43.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.14858672026646028\n",
      "\tValid Loss: 0.14854296131928763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [13:05<01:26, 43.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.14857138955633595\n",
      "\tValid Loss: 0.1485283645292123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [13:48<00:43, 43.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.14856113900533569\n",
      "\tValid Loss: 0.14852927565574647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [14:32<00:00, 43.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.1485492648523501\n",
      "\tValid Loss: 0.1485118014315764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "s2s.apply(init_weights)\n",
    "optimizer = optim.Adam(s2s.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "n_epochs = 20\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "#Testing with subsets\n",
    "#subset_size = 1000\n",
    "#train_subs = training_data[:subset_size]\n",
    "#valid_subs = validation_data[:subset_size]\n",
    "\n",
    "training_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "    train_loss = train_fn(s2s, optimizer, criterion, training_data)\n",
    "    valid_loss = eval_fn(s2s, criterion, validation_data)\n",
    "\n",
    "    training_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    print(f\"\\tTrain Loss: {train_loss}\")\n",
    "    print(f\"\\tValid Loss: {valid_loss}\")\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(s2s.state_dict(), \"saved_lstm.pt\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_english(embeddings, index_to_embedding, index_to_target):\n",
    "    tokens = []\n",
    "    for i in range(len(embeddings)):\n",
    "        minimum = np.Inf\n",
    "        minimum_index = -1\n",
    "        for j in range(len(index_to_embedding)):\n",
    "            if torch.norm(index_to_embedding[j] - embeddings[i]) < minimum:\n",
    "                minimum = torch.norm(index_to_embedding[j] - embeddings[i])\n",
    "                minimum_index = j\n",
    "        tokens.append(index_to_target[minimum_index])\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.50it/s]\n"
     ]
    }
   ],
   "source": [
    "def test(model, test_data, index_to_embedding, index_to_target):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    predicted_tokens = []\n",
    "    actual_tokens = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm.tqdm(range(10)): #TODO::: CHANGE MEEEE\n",
    "            src = test_data[i]['src_embeddings']\n",
    "            tgt = test_data[i]['tgt_embeddings']\n",
    "\n",
    "            output = model(src, tgt, tf=False)\n",
    "            predicted_tokens.append(convert_to_english(output, index_to_embedding, index_to_target))\n",
    "            actual_tokens.append(convert_to_english(test_data[i]['tgt_embeddings'], index_to_embedding, index_to_target))\n",
    "\n",
    "            loss = criterion(output, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss/len(test_data), predicted_tokens, actual_tokens\n",
    "\n",
    "test_loss, predictions, actual = test(s2s, testing_data, index_to_embedding, index_to_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Train/Validation error as a function of epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'subplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m epochs_x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(n_epochs) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 4\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubplot\u001b[49m()\n\u001b[1;32m      5\u001b[0m ax\u001b[38;5;241m.\u001b[39mplot(epochs_x, training_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m ax\u001b[38;5;241m.\u001b[39mplot(epochs_x, valid_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pycourse/lib/python3.8/site-packages/matplotlib/_api/__init__.py:226\u001b[0m, in \u001b[0;36mcaching_module_getattr.<locals>.__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m props:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m props[name]\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance)\n\u001b[0;32m--> 226\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'subplot'"
     ]
    }
   ],
   "source": [
    "import matplotlib as plt\n",
    "epochs_x = np.arange(n_epochs) + 1\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochs_x, training_losses, label=\"Training Loss\")\n",
    "ax.plot(epochs_x, valid_losses, label=\"Validation Loss\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
